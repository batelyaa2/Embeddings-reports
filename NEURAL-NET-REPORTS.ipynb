{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - MLP (multilayer perceptron)\n",
    "This notebook introduces the MNIST handwritten digits dataset and multilayer perceptron classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bge-large-en-v1.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T08:04:03.482170Z",
     "start_time": "2022-08-27T08:04:01.686220Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root_notebook_folder = os.path.abspath('.')\n",
    "data_folder = os.path.abspath('./local_data')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_parquet = 'embeded_final_' + model_name + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T08:04:06.944422Z",
     "start_time": "2022-08-27T08:04:03.482170Z"
    }
   },
   "outputs": [],
   "source": [
    "# file_name = os.path.join(data_folder, 'embeded_final.csv')\n",
    "\n",
    "file_name = os.path.join(data_folder, 'data_for_supervised', model_name_parquet)\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_parquet(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>movement_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.012828</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>-0.023391</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>-0.020759</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>-0.004620</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>0.017626</td>\n",
       "      <td>0.049671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036811</td>\n",
       "      <td>-0.030837</td>\n",
       "      <td>-0.009902</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.047236</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>-0.022012</td>\n",
       "      <td>-0.011921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.013832</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>0.020885</td>\n",
       "      <td>-0.020149</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>-0.003974</td>\n",
       "      <td>-0.007666</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.048920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037093</td>\n",
       "      <td>-0.029989</td>\n",
       "      <td>-0.009730</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.046412</td>\n",
       "      <td>0.024297</td>\n",
       "      <td>-0.021598</td>\n",
       "      <td>-0.022996</td>\n",
       "      <td>-0.013874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.011602</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>-0.021433</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>0.016250</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>-0.008697</td>\n",
       "      <td>0.019624</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035008</td>\n",
       "      <td>-0.027318</td>\n",
       "      <td>-0.010153</td>\n",
       "      <td>0.032859</td>\n",
       "      <td>0.044599</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>-0.020713</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>-0.011632</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.013550</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>0.022362</td>\n",
       "      <td>-0.020639</td>\n",
       "      <td>0.014104</td>\n",
       "      <td>-0.005741</td>\n",
       "      <td>-0.011706</td>\n",
       "      <td>0.018337</td>\n",
       "      <td>0.050350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035246</td>\n",
       "      <td>-0.031524</td>\n",
       "      <td>-0.009279</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.048446</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>-0.020918</td>\n",
       "      <td>-0.020293</td>\n",
       "      <td>-0.011769</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-0.025176</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>-0.020414</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>-0.006912</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>0.017347</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035813</td>\n",
       "      <td>-0.031052</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>-0.019980</td>\n",
       "      <td>-0.022457</td>\n",
       "      <td>-0.012740</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>-0.016140</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>-0.021068</td>\n",
       "      <td>0.014904</td>\n",
       "      <td>-0.024773</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.052986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022750</td>\n",
       "      <td>-0.025471</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.043843</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>-0.016916</td>\n",
       "      <td>-0.019180</td>\n",
       "      <td>-0.002369</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160</th>\n",
       "      <td>-0.015037</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>-0.025471</td>\n",
       "      <td>0.017483</td>\n",
       "      <td>-0.023001</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.048330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025056</td>\n",
       "      <td>-0.028140</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>0.044044</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>-0.020287</td>\n",
       "      <td>-0.020002</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>-0.013757</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>-0.023787</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>-0.026334</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.048968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027970</td>\n",
       "      <td>-0.026673</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.016877</td>\n",
       "      <td>-0.021554</td>\n",
       "      <td>-0.021788</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>-0.018161</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>-0.023106</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>-0.022326</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>0.050792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029186</td>\n",
       "      <td>-0.027653</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>0.034111</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>-0.018968</td>\n",
       "      <td>-0.018492</td>\n",
       "      <td>-0.004089</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>-0.015732</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>-0.024812</td>\n",
       "      <td>0.023475</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>0.010950</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.048722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028023</td>\n",
       "      <td>-0.028703</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.032874</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.016107</td>\n",
       "      <td>-0.023845</td>\n",
       "      <td>-0.018650</td>\n",
       "      <td>-0.003457</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8164 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.012828  0.002089 -0.023391  0.023389 -0.020759  0.014113 -0.004620   \n",
       "1    -0.013832  0.001074 -0.024396  0.020885 -0.020149  0.012370 -0.003974   \n",
       "2    -0.011602  0.003142 -0.021433  0.023721 -0.020300  0.016250 -0.002687   \n",
       "3    -0.013550 -0.001042 -0.025677  0.022362 -0.020639  0.014104 -0.005741   \n",
       "4    -0.013175 -0.000562 -0.025176  0.025241 -0.020414  0.013714 -0.006912   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8159 -0.016140  0.002765 -0.021068  0.014904 -0.024773  0.012019  0.000108   \n",
       "8160 -0.015037  0.001795 -0.025471  0.017483 -0.023001  0.015526  0.001447   \n",
       "8161 -0.013757  0.001451 -0.023787  0.018711 -0.026334  0.016010  0.001308   \n",
       "8162 -0.018161 -0.001021 -0.023106  0.022046 -0.022326  0.014735  0.001045   \n",
       "8163 -0.015732  0.001703 -0.024812  0.023475 -0.024513  0.010950  0.002250   \n",
       "\n",
       "             7         8         9  ...      1015      1016      1017  \\\n",
       "0    -0.008366  0.017626  0.049671  ... -0.036811 -0.030837 -0.009902   \n",
       "1    -0.007666  0.019587  0.048920  ... -0.037093 -0.029989 -0.009730   \n",
       "2    -0.008697  0.019624  0.049712  ... -0.035008 -0.027318 -0.010153   \n",
       "3    -0.011706  0.018337  0.050350  ... -0.035246 -0.031524 -0.009279   \n",
       "4    -0.009624  0.017347  0.048854  ... -0.035813 -0.031052 -0.008171   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8159  0.001141  0.008714  0.052986  ... -0.022750 -0.025471  0.001547   \n",
       "8160  0.003537  0.011914  0.048330  ... -0.025056 -0.028140  0.002017   \n",
       "8161  0.002894  0.010190  0.048968  ... -0.027970 -0.026673  0.001311   \n",
       "8162  0.000673  0.009909  0.050792  ... -0.029186 -0.027653 -0.000698   \n",
       "8163  0.003536  0.009606  0.048722  ... -0.028023 -0.028703  0.002567   \n",
       "\n",
       "          1018      1019      1020      1021      1022      1023  \\\n",
       "0     0.030089  0.047236  0.023684 -0.022334 -0.022012 -0.011921   \n",
       "1     0.032658  0.046412  0.024297 -0.021598 -0.022996 -0.013874   \n",
       "2     0.032859  0.044599  0.022673 -0.020713 -0.022317 -0.011632   \n",
       "3     0.029467  0.048446  0.022119 -0.020918 -0.020293 -0.011769   \n",
       "4     0.031470  0.047521  0.022224 -0.019980 -0.022457 -0.012740   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "8159  0.032937  0.043843  0.016535 -0.016916 -0.019180 -0.002369   \n",
       "8160  0.033176  0.044044  0.013970 -0.020287 -0.020002 -0.002353   \n",
       "8161  0.034371  0.045282  0.016877 -0.021554 -0.021788 -0.000572   \n",
       "8162  0.034111  0.043050  0.015890 -0.018968 -0.018492 -0.004089   \n",
       "8163  0.032874  0.042914  0.016107 -0.023845 -0.018650 -0.003457   \n",
       "\n",
       "      movement_label  \n",
       "0                NaN  \n",
       "1                1.0  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "8159             NaN  \n",
       "8160             NaN  \n",
       "8161             NaN  \n",
       "8162             0.0  \n",
       "8163             1.0  \n",
       "\n",
       "[8164 rows x 1025 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['movement_label'] = data['movement_label'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2.0\n",
       "1       1.0\n",
       "2       2.0\n",
       "3       2.0\n",
       "4       1.0\n",
       "       ... \n",
       "8159    2.0\n",
       "8160    2.0\n",
       "8161    2.0\n",
       "8162    0.0\n",
       "8163    1.0\n",
       "Name: movement_label, Length: 8164, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['movement_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len_x = len(data.columns) - 1\n",
    "X = np.vstack(data.values[:,0:len_x] ).astype(np.float32)\n",
    "y = data[\"movement_label\"].astype(int).values\n",
    "\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (8164, 1024) classes: [0 1 2]\n",
      "Epoch 001 | loss=1.1336933374404907 | \n",
      "Epoch 002 | loss=0.9895453453063965 | \n",
      "Epoch 003 | loss=1.0620204210281372 | \n",
      "Epoch 004 | loss=1.0693849325180054 | \n",
      "Epoch 005 | loss=0.986126720905304 | \n",
      "Epoch 006 | loss=1.0886253118515015 | \n",
      "Epoch 007 | loss=0.9567194581031799 | \n",
      "Epoch 008 | loss=1.1339327096939087 | \n",
      "Epoch 009 | loss=0.8809139132499695 | \n",
      "Epoch 010 | loss=0.7725961804389954 | \n",
      "Epoch 011 | loss=0.9218748807907104 | \n",
      "Epoch 012 | loss=0.971961498260498 | \n",
      "Epoch 013 | loss=0.8989092111587524 | \n",
      "Epoch 014 | loss=0.662523627281189 | \n",
      "Epoch 015 | loss=0.6461701989173889 | \n",
      "Epoch 016 | loss=0.6011753678321838 | \n",
      "Epoch 017 | loss=0.8593766689300537 | \n",
      "Epoch 018 | loss=0.909336507320404 | \n",
      "Epoch 019 | loss=0.6443436741828918 | \n",
      "Epoch 020 | loss=0.663413941860199 | \n",
      "\n",
      "TEST | loss=1.0855 | acc=0.5200 | f1_macro=0.3702\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 45  43  87]\n",
      " [ 43  46  83]\n",
      " [176 156 546]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1705    0.2571    0.2050       175\n",
      "           1     0.1878    0.2674    0.2206       172\n",
      "           2     0.7626    0.6219    0.6851       878\n",
      "\n",
      "    accuracy                         0.5200      1225\n",
      "   macro avg     0.3736    0.3822    0.3702      1225\n",
      "weighted avg     0.5973    0.5200    0.5513      1225\n",
      "\n",
      "\n",
      "Saved: best_mlp.pth and scaler.json\n"
     ]
    }
   ],
   "source": [
    "# pip install torch scikit-learn pandas numpy\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ====== 1) קלט: השתמשו ב-df קיים, או טענו מקובץ ======\n",
    "# אם כבר יש לכם df בזיכרון, השאירו שורה זו כמובנת מאליה.\n",
    "# אחרת, בטלו הערה לשורה הבאה והכניסו את הנתיב:\n",
    "# df = pd.read_csv(\"your_embeddings.csv\")\n",
    "\n",
    "# שמות עמודות (התאימו אם אצלכם שונים)\n",
    "EMB_COL = \"embedding\"\n",
    "LABEL_COL = \"movement_label\"\n",
    "\n",
    "# אם ה-embedding נשמר כמחרוזת, נהפוך לרשימה\n",
    "\n",
    "\n",
    "\n",
    "# המרת ל-numpy\n",
    "len_x = len(data.columns) - 1\n",
    "X = np.vstack(data.values[:,0:len_x] ).astype(np.float32)\n",
    "y = data[\"movement_label\"].astype(int).values\n",
    "\n",
    "# (אופציונלי) אם יש NaN בלייבל ואתם רוצים להחליף ל-2:\n",
    "# y = np.where(pd.isna(y), 2, y).astype(int)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Shape X:\", X.shape, \"classes:\", np.unique(y))\n",
    "\n",
    "# ====== 2) פיצול ל-train/val/test בשכבות ======\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ====== 3) נרמול/סטנדרטיזציה (למרות ש-embeddings לרוב מנורמלים) ======\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# ====== 4) PyTorch Dataset/DataLoader ======\n",
    "class NPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = NPDataset(X_train, y_train)\n",
    "val_ds   = NPDataset(X_val, y_val)\n",
    "test_ds  = NPDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# ====== 5) מודל MLP פשוט ======\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_dim, num_classes).to(device)\n",
    "\n",
    "# ====== 6) איזון מחלקות (class weights) ======\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "class_weights = (len(y_train) / (num_classes * np.clip(class_counts, 1, None))).astype(np.float32)\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# ====== 7) לולאת אימון עם Early Stopping ======\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "EPOCHS = 20\n",
    "\n",
    "def eval_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_y = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * yb.size(0)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_y.append(yb.cpu())\n",
    "    val_loss /= len(loader.dataset)\n",
    "    y_true = torch.cat(all_y).numpy()\n",
    "    y_pred = torch.argmax(torch.cat(all_logits), dim=1).numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return val_loss, acc, f1m\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * yb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:03d} | loss={loss} | \")\n",
    "\n",
    "    val_loss, val_acc, val_f1 = eval_loader(model, val_loader)\n",
    "    # print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | val_f1={val_f1:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_mlp.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# ====== 8) טעינת המודל הטוב ובדיקה על ה-Test ======\n",
    "model.load_state_dict(torch.load(\"best_mlp.pth\", map_location=device))\n",
    "\n",
    "test_loss, test_acc, test_f1 = eval_loader(model, test_loader)\n",
    "print(f\"\\nTEST | loss={test_loss:.4f} | acc={test_acc:.4f} | f1_macro={test_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix + דו\"ח\n",
    "model.eval()\n",
    "y_true_all, y_pred_all = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb).cpu()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_true_all.append(yb)\n",
    "        y_pred_all.append(preds)\n",
    "\n",
    "y_true_all = torch.cat(y_true_all).numpy()\n",
    "y_pred_all = torch.cat(y_pred_all).numpy()\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true_all, y_pred_all))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true_all, y_pred_all, digits=4))\n",
    "\n",
    "# ====== 9) שמירת scaler לפרודקשן ======\n",
    "# שמירה פשוטה של הפרמטרים של ה-StandardScaler\n",
    "scaler_artifact = {\n",
    "    \"mean_\": scaler.mean_.tolist(),\n",
    "    \"scale_\": scaler.scale_.tolist(),\n",
    "}\n",
    "with open(\"scaler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scaler_artifact, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nSaved: best_mlp.pth and scaler.json\")\n",
    "\n",
    "# ====== 10) פונקציית חיזוי לשימוש בהמשך ======\n",
    "def load_scaler(path=\"scaler.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    sc = StandardScaler()\n",
    "    sc.mean_ = np.array(obj[\"mean_\"], dtype=np.float64)\n",
    "    sc.scale_ = np.array(obj[\"scale_\"], dtype=np.float64)\n",
    "    sc.n_features_in_ = sc.mean_.shape[0]\n",
    "    return sc\n",
    "\n",
    "def predict_embeddings(emb_list, model_path=\"best_mlp.pth\", scaler_path=\"scaler.json\"):\n",
    "    \"\"\"\n",
    "    emb_list: list of embedding vectors (each is list/np.array of length d)\n",
    "    returns: np.array of predicted class ids\n",
    "    \"\"\"\n",
    "    sc = load_scaler(scaler_path)\n",
    "    Xn = sc.transform(np.array(emb_list, dtype=np.float32))\n",
    "    mdl = MLP(Xn.shape[1], num_classes)\n",
    "    mdl.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = mdl(torch.from_numpy(Xn).float())\n",
    "        preds = torch.argmax(logits, dim=1).numpy()\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_embeddings(X_test, model_path=\"best_mlp.pth\", scaler_path=\"scaler.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "סה\"כ טסט: 1225\n",
      "סה״כ כניסות שוות: 528\n",
      "אחוז הצלחה כללי: 43.10204081632653\n",
      "שניהם 0: 63\n",
      "שניהם 1: 48\n",
      "שניהם 2: 417\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('סה\"כ טסט:', len(y_test))\n",
    "# ספירת שוויון כולל\n",
    "total_equal = np.sum(y_pred == y_test)\n",
    "\n",
    "# ספירה לפי ערכים\n",
    "equal_0 = np.sum((y_pred == 0) & (y_test == 0))\n",
    "equal_1 = np.sum((y_pred == 1) & (y_test == 1))\n",
    "equal_2 = np.sum((y_pred == 2) & (y_test == 2))\n",
    "pct = (total_equal / len(y_pred ) ) * 100\n",
    "\n",
    "print(\"סה״כ כניסות שוות:\", total_equal)\n",
    "print(\"אחוז הצלחה כללי:\" , pct)\n",
    "print(\"שניהם 0:\", equal_0)\n",
    "print(\"שניהם 1:\", equal_1)\n",
    "print(\"שניהם 2:\", equal_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 -> ACC=0.5168 | F1_macro=0.3649\n",
      "Fold 2/5 -> ACC=0.4985 | F1_macro=0.3574\n",
      "Fold 3/5 -> ACC=0.5070 | F1_macro=0.3782\n",
      "Fold 4/5 -> ACC=0.5321 | F1_macro=0.3795\n"
     ]
    }
   ],
   "source": [
    "# pip install torch scikit-learn pandas numpy\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ===== קלט =====\n",
    "# df = pd.read_csv(\"your_embeddings.csv\")  # אם צריך לקרוא מקובץ\n",
    "EMB_COL = \"embedding\"\n",
    "LABEL_COL = \"movement_label\"\n",
    "\n",
    "len_x = len(data.columns) - 1\n",
    "X = np.vstack(data.values[:,0:len_x]).astype(np.float32)\n",
    "y = data[LABEL_COL].astype(float).values\n",
    "# אופציונלי: החלפת NaN ב-2\n",
    "# y = np.where(np.isnan(y), 2, y)\n",
    "y = y.astype(int)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ===== DataSet קטן =====\n",
    "class NPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# ===== מודל =====\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),   nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def train_one_fold(X_tr, y_tr, X_va, y_va, max_epochs=100, lr=1e-6, wd=1e-4, patience=20):\n",
    "    # איזון מחלקות לפי ה-train של הפולד\n",
    "    class_counts = np.bincount(y_tr, minlength=num_classes)\n",
    "    class_weights = (len(y_tr) / (num_classes * np.clip(class_counts, 1, None))).astype(np.float32)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(NPDataset(X_tr, y_tr), batch_size=64, shuffle=True)\n",
    "    val_loader   = DataLoader(NPDataset(X_va, y_va), batch_size=256, shuffle=False)\n",
    "\n",
    "    model = MLP(input_dim, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=device))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience_count = 0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, max_epochs+1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            opt.step()\n",
    "            running += loss.item() * yb.size(0)\n",
    "\n",
    "        # ---- val\n",
    "        model.eval()\n",
    "        val_loss, y_true, y_pred = 0.0, [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_loss += loss.item() * yb.size(0)\n",
    "                y_true.append(yb.cpu().numpy())\n",
    "                y_pred.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val - 1e-4:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count >= patience:\n",
    "                break\n",
    "\n",
    "    # שחזור best\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# ===== Cross Validation =====\n",
    "k = 1  # אפשר להגדיל/להקטין\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "accs, f1s = [], []\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, y_tr = X[idx_tr], y[idx_tr]\n",
    "    X_va, y_va = X[idx_va], y[idx_va]\n",
    "\n",
    "    # סטנדרטיזציה בכל פולד (פיט רק על train!)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_tr_n = scaler.fit_transform(X_tr)\n",
    "    X_va_n = scaler.transform(X_va)\n",
    "\n",
    "    model = train_one_fold(X_tr_n, y_tr, X_va_n, y_va)\n",
    "\n",
    "    # הערכה סופית בפולד\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.from_numpy(X_va_n).float().to(device)).cpu().numpy()\n",
    "    y_pred = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(y_va, y_pred)\n",
    "    f1m = f1_score(y_va, y_pred, average=\"macro\")\n",
    "    accs.append(acc); f1s.append(f1m)\n",
    "    print(f\"Fold {fold}/{k} -> ACC={acc:.4f} | F1_macro={f1m:.4f}\")\n",
    "\n",
    "print(\"\\n=== CV summary ===\")\n",
    "print(f\"ACC: mean={np.mean(accs):.4f}  std={np.std(accs):.4f}\")\n",
    "print(f\"F1m: mean={np.mean(f1s):.4f}  std={np.std(f1s):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(torch.from_numpy(X_test).float().to(device)).cpu().numpy()\n",
    "y_pred = logits.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "סה\"כ כניסות: 1232\n",
      "סה״כ כניסות שוות: 701\n",
      "אחוז הצלחה כללי: 56.899350649350644\n",
      "שניהם 0: 115\n",
      "שניהם 1: 63\n",
      "שניהם 2: 523\n"
     ]
    }
   ],
   "source": [
    "# ספירת שוויון כולל\n",
    "total_equal = np.sum(y_pred == y_test)\n",
    "\n",
    "# ספירה לפי ערכים\n",
    "equal_0 = np.sum((y_pred == 0) & (y_test == 0))\n",
    "equal_1 = np.sum((y_pred == 1) & (y_test == 1))\n",
    "equal_2 = np.sum((y_pred == 2) & (y_test == 2))\n",
    "pct = (total_equal / len(y_pred ) ) * 100\n",
    "\n",
    "print('סה\"כ כניסות:',len(y_pred ))\n",
    "print(\"סה״כ כניסות שוות:\", total_equal)\n",
    "print(\"אחוז הצלחה כללי:\" , pct)\n",
    "print(\"שניהם 0:\", equal_0)\n",
    "print(\"שניהם 1:\", equal_1)\n",
    "print(\"שניהם 2:\", equal_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283.724px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
